{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36dd2302",
   "metadata": {},
   "source": [
    "<h1>Decision Trees - Universal Approximators</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6204d04c",
   "metadata": {},
   "source": [
    "<h2>1. Introduction </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdfe3a",
   "metadata": {},
   "source": [
    "Decision trees are universal approximators that use recursive partitioning to divide the datasets into homogenous subgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b4cff",
   "metadata": {},
   "source": [
    "<img src=\"media/decision_trees_.png\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3d975",
   "metadata": {},
   "source": [
    "The <b>top node</b> is referred to as the <b>root node</b> and is the starting decision node. (i.e., Gender is Male or Female?). A <b>branch</b> is a subset of the dataset obtained as an outcome of a test. <b>Internal nodes</b> are decision nodes based on which subsequent branches are obtained. The <b>depth</b> of a node is the minimum number of decisions it takes to reach it from the root node. The leaf nodes are the end of the last branches on the tree which determine the output (class label or regression value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b734b45",
   "metadata": {},
   "source": [
    "<h2>2. Building a decision tree</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad16e0",
   "metadata": {},
   "source": [
    "Given a dataset of <b>n features and m records</b>, a rule-based graph is formed <b>iteratively by recursive partitioning</b> until the datasets is split in homogenous data groups representing the <b>same target class</b> in a classification problem or <b>sharing close target values</b> in a regression problem ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37138914",
   "metadata": {},
   "source": [
    "1. From the root node (i.e. with all the m records), the most informative attribute is identified using some feature important score. The <b>Gini index</b> is the most commonly used feature importance score among others (entropy, information gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d692141a",
   "metadata": {},
   "source": [
    "$$ Gini(f) = \\sum_{i=1}^{N_c}P(class=i|f)(1-P(class=i|f))Â  = 1 - \\sum_{i=1}^{N_c}P(class=i|f)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb64462",
   "metadata": {},
   "source": [
    "Overall Gini coefficient:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce497ae",
   "metadata": {},
   "source": [
    "$$\n",
    "Gini(f) = \\frac{n_{S_i}}{n_{S_i}+n_{S_j}}Gini(f_{S_i}) + \\frac{n_{S_j}}{n_{S_i}+n_{S_j}}Gini(f_{S_j})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b0fd2",
   "metadata": {},
   "source": [
    "<b>The feature with the lowest gini index is selected</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ac220",
   "metadata": {},
   "source": [
    "For a regression problem, the quality of the split is typically measured using the mean squre error:\n",
    "\n",
    "$$\n",
    "\\bar{y} = \\frac{1}{n_{S_i}}\\sum_{y\\in S_i}^{}y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234ae10",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE(S_i) = \\frac{1}{n_{S_i}}\\sum_{i=1}^{n_{S_i}}(\\bar{y}-y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35f914",
   "metadata": {},
   "source": [
    "2- Given an appropriate feature importance selection criterion, the decision tree is thus built as follows by recursive partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e416e",
   "metadata": {},
   "source": [
    "<b>Decision Tree Pseudo-code</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3704b6",
   "metadata": {},
   "source": [
    "Step 1: Given M attributes in a dataset N records and a target variable y<br/>\n",
    "Step 2: Rank features as per the chosen feature importance score<br/>\n",
    "Step 3: Split the dataset by the feature with the best importance score<br/>\n",
    "Step 4: Repeat Step 2 to each new subset until a stopping criterion is met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8233dd08",
   "metadata": {},
   "source": [
    "<h3>3. Pruning a decision tree</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9b633",
   "metadata": {},
   "source": [
    "A decision tree can reach 100% fitting accuracy on the training set given that it can further split the data until a single data (i.e. guaranteed homogeneity) remains. However, this comes with the risk that the algorithm may lose its generalisation capability on unseen data. A pruning phase may post-process the decision tree, undermine some rules and allow some level of heterogeneity in the data subgroups to secure generalisation on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6abbce",
   "metadata": {},
   "source": [
    "<h2>Python Implementation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7911b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../datasets/Healthcare-Diabetes.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
