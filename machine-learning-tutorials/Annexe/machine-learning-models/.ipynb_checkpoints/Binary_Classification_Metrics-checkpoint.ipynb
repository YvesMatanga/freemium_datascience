{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Metrics\n",
    "\n",
    "© Explore Data Science Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "In this train you will learn how to:\n",
    "\n",
    "- Assess the performance of a binary classifier;\n",
    "- Understand how to assess performance in the presence of class imbalance; and \n",
    "- Understand the terms accuracy, precision, recall, and F1-score.\n",
    "\n",
    "## Outline\n",
    "\n",
    "This train is structured as follows:\n",
    "\n",
    "1. Rebuild the logistic regression model from the previous tutorial;\n",
    "2. Assess model performance using the confusion matrix; and\n",
    "3. Assess model performance using the classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Building Our Logistic Regression Model\n",
    "\n",
    "We'll need a trained model in order to asses its performance. We'll use a classic binary classification dataset for this task: the Wisconsin Breast Cancer dataset. It consists of 569 observations with 30 predictors and a single binary response variable.\n",
    "\n",
    "Each observation is the result of a scan on a mass of breast tissue for the purpose of diagnosing or dismissing breast cancer in a patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# import dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# suppress cell warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable, which we'll store in the `DataFrame` _y_, consists of two classes, each referring to the diagnosis of a scan of a mass of breast tissue:\n",
    "\n",
    "- 1: the mass is benign;\n",
    "- 0: the mass is malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframes\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y, columns=['Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the 30 predictor variables. All continuous, all encoded to four significant digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3        4        5       6        7       8   \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        9   ...     20     21      22      23      24      25      26      27  \\\n",
       "0  0.07871  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.05667  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.05999  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.09744  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.05883  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       28       29  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, we'll take a look at the distribution of observations between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    357\n",
       "0    212\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "\n",
    "plt.title('Class Frequency')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(ticks=[0,1], labels=[0,1])\n",
    "plt.ylim(top=300)\n",
    "\n",
    "plt.show()\n",
    "y['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note of the imbalance here: there are 357 observations in class 1 and only 212 in class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logistic regression model instance\n",
    "lm = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10066666580236579"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.593849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.278820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.627084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.032688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.017369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Coefficient\n",
       "0     0.593849\n",
       "1     0.278820\n",
       "2     0.627084\n",
       "3    -0.032688\n",
       "4    -0.017369"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_df = pd.DataFrame(lm.coef_.T,X.columns,columns=['Coefficient'])\n",
    "coeff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lm = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we've made some predictions. Now we can begin to compare those predictions to the ground truth labels of the test dataset, and determine how well the model has actually done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Assessing Model Performance using the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is often used to describe the performance of a classification model, or _classifier_ , on a set of test data for which the true values are known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-conf-matrix.png\" alt=\"sketch-conf-matrix\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing. Below are the basic terms, which are whole numbers (not proportions), each referring to a separate cell in the confusion matrix, above:\n",
    "\n",
    "- **True negatives (TN)**: These are cases in which we predicted no claim, and there was no claim;\n",
    "- **True positives (TP)**: We predicted a claim, and they did indeed claim;\n",
    "- **False positives (FP)**: We predicted a claim (positive), when there was none (false). Also known as a **Type I error**;\n",
    "- **False negatives (FN)**: We predicted no claim, but they actually did claim. Also known as a **Type II error**.\n",
    "\n",
    "Take a moment to familiarise yourself with the table and how we arrive at each of the four categories, using the sketch below to take note of which prediction falls into which category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-compare-predictions.png\" alt=\"sketch-compare-predictions\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix in `sklearn`\n",
    "\n",
    "Let's import the `confusion_matrix` object to check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix takes in two arguments: the unseen test data `y_test`, as well as our predictions, `pred_lm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36,  3],\n",
       "       [ 5, 70]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look very nice - let's convert it into a dataframe and add the appropriate labels to make it clear which value is which.\n",
    "\n",
    "The matrix orders the rows and columns in a sorted fashion according to the labels. Our labels are 0 and 1, so the first row/column is 0, and the 2nd row/column is 1. Let's give it the appropriate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0: Malignant</th>\n",
       "      <th>1: Benign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0: Malignant</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1: Benign</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0: Malignant  1: Benign\n",
       "0: Malignant            36          3\n",
       "1: Benign                5         70"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['0: Malignant', '1: Benign']\n",
    "\n",
    "pd.DataFrame(data=confusion_matrix(y_test, pred_lm), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks a little better. A few notes on the matrix:\n",
    "\n",
    "- Each row represents the ground truth totals for _Malignant_ and _Benign_. In other words, the sum of all the values in the first row is the total number of observations in our test dataset labelled _Malignant_.\n",
    "\n",
    "- Each column represents the totals for the predictions in each of _Malignant_ and _Benign_.\n",
    "\n",
    "- The intersection of each row/column gives us a different aspect of the results: TP, TN, FP, or FN, as described for the table sketched above.\n",
    "\n",
    "- Based on the confusion matrix shown here, the model classified $36 + 5 = 41$ observations to be _Malignant_, and $70 + 3 = 73$ to be _Benign_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Accuracy\n",
    "\n",
    "The results shown above lead us to our first classification metric: **overall accuracy**, which we calculate according to the following formula:\n",
    "\n",
    "$$Accuracy =  \\frac{Correct\\space predictions}{Total\\space predictions} = \\frac{TP + \\space TN}{TP \\space + \\space TN \\space + \\space FP \\space + \\space FN}$$\n",
    "\n",
    "Our overall accuracy is calculated as follows:\n",
    "\n",
    "$$Accuracy =  \\frac{Correct\\space predictions}{Total\\space predictions} = \\frac{70 + 36}{70 + 36 + 3 + 5} = 0.93$$\n",
    "\n",
    "At first glance this appears to a useful, catch-all metric which tells us everything we need to know about our model. The problem is that it lacks detail.\n",
    "\n",
    "Consider the following scenario:\n",
    "\n",
    "- We have 100 observations in our test dataset: 90 of them are labelled _No_ , the remaining 10 labelled _Yes_. \n",
    "\n",
    "- At prediction time, our model classifies all 100 observations to be in category _No_. Our model made 100 predictions, and got all 90 of the _No_ observations correct, giving it an overall accuracy of 90%!\n",
    "\n",
    "- Sounds good right? The problem is that the model got literally none of the _Yes_-labelled observations correct - 0/10! What if the _Yes_ cases were for patients have cancer, or a transaction that is fraudulent? Those are important results, and we would have missed all of them.\n",
    "\n",
    "- Hopefully, that has highlighted the importance of being accurate not just overall, but in each particular class too.\n",
    "\n",
    "Let's look at few metrics which are a little more comprehensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assessing Model Performance using the Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Classification Report gives us more information on where our model is going wrong - looking specifically at the performance caused by Type I & II errors.  The following metrics are calculated as part of the classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "When it predicts _yes_, how often is it correct? \n",
    "\n",
    "$$ Precision = \\frac{TP}{TP \\space + FP} = \\frac{TP}{Total \\space Predicted \\space Positive} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "\n",
    "When the outcome is actually _yes_, how often do we predict it as such?\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP \\space + FN} = \\frac{TP}{Total \\space Actual \\space Positive}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score\n",
    "\n",
    "Weighted average of precision and recall. \n",
    "\n",
    "$$F_1 = 2 \\times \\frac {Precision \\space \\times \\space Recall }{Precision \\space + \\space Recall }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall _and_ there is an uneven class distribution (large number of 1s vs 0s or vice versa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the `classification_report` object to check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the confusion matrix, the classification matrix takes in two arguments: the unseen y_test data as well as our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "0: Malignant       0.88      0.92      0.90        39\n",
      "   1: Benign       0.96      0.93      0.95        75\n",
      "\n",
      "    accuracy                           0.93       114\n",
      "   macro avg       0.92      0.93      0.92       114\n",
      "weighted avg       0.93      0.93      0.93       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(classification_report(y_test, pred_lm, target_names=['0: Malignant', '1: Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "We now have a far more comprehensive view of the performance of our model.\n",
    "\n",
    "- Clearly, the precision, recall and f1-score values for the benign class are higher, and this has to do with the class imbalance we referred to earlier in the tutorial. There are more observations with the benign label, so the model gets _better_ at classifiying those ones because it has more evidence of them.\n",
    "\n",
    "- The corresponding values in the malignant class are lower for the same reason.\n",
    "\n",
    "- The weighted f1-score here gives us a good indication using a single value of how well the model is performed. It is somewhere between the accuracies that the model achieved for each of class 0 and 1, but slightly in favour of class 1, of which there were more examples.\n",
    "\n",
    "- Perhaps the most important information in the above table is in the last row, indicating the weighted average. unlike the values in the `macro avg` row which are computed using: $\\frac{class\\_0\\_metric \\quad + \\quad class\\_1\\_metric}{2}$ , the `weighted avg` values are computed using: $\\frac{class\\_0\\_metric \\, \\times \\, \\%\\_class\\_0\\_labels  \\quad + \\quad class\\_1\\_metric \\, \\times \\, \\%\\_class\\_1\\_labels}{2}$ , which takes into account the proportions of each class fed into the model (as indicated in the support column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train we have seen or been introduced to:\n",
    "\n",
    "- Using the confusion matrix to assess the classifications from a binary classifier;\n",
    "- Understanding the four result categories a classification may fall into (TP, TN, etc);\n",
    "- Four metrics for assessing a classifier: accuracy, precision, recall, and F1-score;\n",
    "- The importance of ensuring good performance in _each class_, as opposed to just overall accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
