{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36dd2302",
   "metadata": {},
   "source": [
    "<h1>Decision Trees - Learning Nonlinearities using Rules</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b108ce4",
   "metadata": {},
   "source": [
    "<b>Outline</b>\n",
    "<ul>\n",
    "    <li>Decision Trees: Concept</li>\n",
    "    <li>Decision Trees: Examining nonlinearity learning</li>\n",
    "    <li>Decision Trees: Boosting generalisation via pruning</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6204d04c",
   "metadata": {},
   "source": [
    "<h2>1. Introduction </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdfe3a",
   "metadata": {},
   "source": [
    "Decision trees are universal approximators that use recursive partitioning to divide the datasets into homogenous subgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b4cff",
   "metadata": {},
   "source": [
    "<img src=\"../Regression/media/decision_trees_.png\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3d975",
   "metadata": {},
   "source": [
    "The <b>top node</b> is referred to as the <b>root node</b> and is the starting decision node. (i.e., Gender is Male or Female?). A <b>branch</b> is a subset of the dataset obtained as an outcome of a test. <b>Internal nodes</b> are decision nodes based on which subsequent branches are obtained. The <b>depth</b> of a node is the minimum number of decisions it takes to reach it from the root node. The leaf nodes are the end of the last branches on the tree which determine the output (class label or regression value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b734b45",
   "metadata": {},
   "source": [
    "<h2>2. Building a decision tree</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad16e0",
   "metadata": {},
   "source": [
    "Given a dataset of <b>n features and m records</b>, a rule-based graph is formed <b>iteratively by recursive partitioning</b> until the datasets is split in homogenous data groups representing the <b>same target class</b> in a classification problem or <b>sharing close target values</b> in a regression problem ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37138914",
   "metadata": {},
   "source": [
    "1. From the root node (i.e. with all the m records), the most informative attribute is identified using some feature important score. The <b>Gini index</b> is the most commonly used feature importance score among others (entropy, information gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d692141a",
   "metadata": {},
   "source": [
    "$$ Gini(f) = \\sum_{i=1}^{N_c}P(class=i|f)(1-P(class=i|f))Â  = 1 - \\sum_{i=1}^{N_c}P(class=i|f)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb64462",
   "metadata": {},
   "source": [
    "Overall Gini coefficient:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce497ae",
   "metadata": {},
   "source": [
    "$$\n",
    "Gini(f) = \\frac{n_{S_i}}{n_{S_i}+n_{S_j}}Gini(f_{S_i}) + \\frac{n_{S_j}}{n_{S_i}+n_{S_j}}Gini(f_{S_j})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b0fd2",
   "metadata": {},
   "source": [
    "<b>The feature with the lowest gini index is selected</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ac220",
   "metadata": {},
   "source": [
    "For a regression problem, the quality of the split is typically measured using the mean squre error:\n",
    "\n",
    "$$\n",
    "\\bar{y} = \\frac{1}{n_{S_i}}\\sum_{y\\in S_i}^{}y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234ae10",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE(S_i) = \\frac{1}{n_{S_i}}\\sum_{i=1}^{n_{S_i}}(\\bar{y}-y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35f914",
   "metadata": {},
   "source": [
    "2- Given an appropriate feature importance selection criterion, the decision tree is thus built as follows by recursive partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e416e",
   "metadata": {},
   "source": [
    "<b>Decision Tree Pseudo-code</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3704b6",
   "metadata": {},
   "source": [
    "Step 1: Given M attributes in a dataset N records and a target variable y<br/>\n",
    "Step 2: Rank features as per the chosen feature importance score<br/>\n",
    "Step 3: Split the dataset by the feature with the best importance score<br/>\n",
    "Step 4: Repeat Step 2 to each new subset until a stopping criterion is met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8233dd08",
   "metadata": {},
   "source": [
    "<h3>3. Pruning a decision tree</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9b633",
   "metadata": {},
   "source": [
    "A decision tree can reach 100% fitting accuracy on the training set given that it can further split the data until a single data (i.e. guaranteed homogeneity) remains. However, this comes with the risk that the algorithm may lose its generalisation capability on unseen data. A pruning phase may post-process the decision tree, undermine some rules and allow some level of heterogeneity in the data subgroups to secure generalisation on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6abbce",
   "metadata": {},
   "source": [
    "<h2>Python Implementation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "256112a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0   1            6      148             72             35        0  33.6   \n",
       "1   2            1       85             66             29        0  26.6   \n",
       "2   3            8      183             64              0        0  23.3   \n",
       "3   4            1       89             66             23       94  28.1   \n",
       "4   5            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../datasets/Healthcare-Diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42d28375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:, 1:9]#get features\n",
    "y = df.iloc[:,[-1]]#get target variable\n",
    "\n",
    "#---Data Scaling\n",
    "Sc = StandardScaler()\n",
    "Sc.fit(X)\n",
    "X_d = Sc.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14731cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Train - Test Split\n",
    "X_train, X_test,y_train,y_test = train_test_split(X_d,y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4f45831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'min_samples_leaf': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dt_parameters = {'max_depth':[None, 8, 10], 'min_samples_leaf':[1,2,3]}\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "clf = GridSearchCV(dt, dt_parameters,cv=5)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e18d4901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Trees - Test Performance: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " no-diabetes       0.99      1.00      0.99       349\n",
      "has-diabetes       1.00      0.98      0.99       205\n",
      "\n",
      "    accuracy                           0.99       554\n",
      "   macro avg       0.99      0.99      0.99       554\n",
      "weighted avg       0.99      0.99      0.99       554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "class_acc_dt = metrics.accuracy_score(y_test,y_pred_test)#get classification accuracy\n",
    "\n",
    "targets = ['no-diabetes','has-diabetes']\n",
    "print(\"Decision Trees - Test Performance: \\n\",classification_report(y_test,y_pred_test,target_names=targets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
